{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pk\n",
    "import os \n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, OrderedDict\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "# from options import args\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following re patterns and cleaning processes are adapted from the biowordvec repo\n",
    "# ==============================================================================\n",
    "SECTION_TITLES = re.compile(\n",
    "    r'('\n",
    "    r'ABDOMEN AND PELVIS|CLINICAL HISTORY|CLINICAL INDICATION|COMPARISON|COMPARISON STUDY DATE'\n",
    "    r'|EXAM|EXAMINATION|FINDINGS|HISTORY|IMPRESSION|INDICATION'\n",
    "    r'|MEDICAL CONDITION|PROCEDURE|REASON FOR EXAM|REASON FOR STUDY|REASON FOR THIS EXAMINATION'\n",
    "    r'|TECHNIQUE'\n",
    "    r'):|FINAL REPORT',\n",
    "    re.I | re.M)\n",
    "\n",
    "\n",
    "def pattern_repl(matchobj):\n",
    "    \"\"\"\n",
    "    Return a replacement string to be used for match object\n",
    "    \"\"\"\n",
    "    return ' '.rjust(len(matchobj.group(0)))\n",
    "\n",
    "\n",
    "def find_end(text):\n",
    "    \"\"\"Find the end of the report.\"\"\"\n",
    "    ends = [len(text)]\n",
    "    patterns = [\n",
    "        re.compile(r'BY ELECTRONICALLY SIGNING THIS REPORT', re.I),\n",
    "        re.compile(r'\\n {3,}DR.', re.I),\n",
    "        re.compile(r'[ ]{1,}RADLINE ', re.I),\n",
    "        re.compile(r'.*electronically signed on', re.I),\n",
    "        re.compile(r'M\\[0KM\\[0KM')\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        matchobj = pattern.search(text)\n",
    "        if matchobj:\n",
    "            ends.append(matchobj.start())\n",
    "    return min(ends)\n",
    "\n",
    "\n",
    "def split_heading(text):\n",
    "    \"\"\"Split the report into sections\"\"\"\n",
    "    start = 0\n",
    "    for matcher in SECTION_TITLES.finditer(text):\n",
    "        # add last\n",
    "        end = matcher.start()\n",
    "        if end != start:\n",
    "            section = text[start:end].strip()\n",
    "            if section:\n",
    "                yield section\n",
    "\n",
    "        # add title\n",
    "        start = end\n",
    "        end = matcher.end()\n",
    "        if end != start:\n",
    "            section = text[start:end].strip()\n",
    "            if section:\n",
    "                yield section\n",
    "\n",
    "        start = end\n",
    "\n",
    "    # add last piece\n",
    "    end = len(text)\n",
    "    if start < end:\n",
    "        section = text[start:end].strip()\n",
    "        if section:\n",
    "            yield section\n",
    "\n",
    "# PREPROCESAMIENTO\n",
    "# Token\n",
    "# Spell checker\n",
    "# Lematización Inglés             \n",
    "# Ingles Lemas -> Vector\n",
    "# Traducción Lemas ingles-español\n",
    "# Guardar ese embedding Español -> Vector\n",
    "# Guardar ese embedding Ingles -> Vector\n",
    "        \n",
    "# PRODUCCIÓN\n",
    "# Texto Español\n",
    "# Token -> Spell Español -> Lematización -> Vector  \n",
    "            \n",
    "\n",
    "# ENTRENAMIENTO MIMIC III v1.4\n",
    "# Filtrar DRGS \n",
    "# Entrenar Modelo \n",
    "# Ingles -> Vector -> Modelo -> DRG                  \n",
    "            \n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace [**Patterns**] with spaces.\n",
    "    text = re.sub(r'\\[\\*\\*.*?\\*\\*\\]', pattern_repl, text)\n",
    "    # Replace `_` with spaces.\n",
    "    text = re.sub(r'_', ' ', text)\n",
    "\n",
    "    start = 0\n",
    "    end = find_end(text)\n",
    "    new_text = ''\n",
    "    if start > 0:\n",
    "        new_text += ' ' * start\n",
    "    new_text = text[start:end]\n",
    "\n",
    "    # make sure the new text has the same length of old text.\n",
    "    if len(text) - end > 0:\n",
    "        new_text += ' ' * (len(text) - end)\n",
    "    return new_text\n",
    "\n",
    "def preprocess_mimic(text):\n",
    "    \"\"\"\n",
    "    Preprocess reports in MIMIC-III.\n",
    "    1. remove [**Patterns**] and signature\n",
    "    2. split the report into sections\n",
    "    3. tokenize sentences and words\n",
    "    4. lowercase\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for sec in split_heading(clean_text(text)):\n",
    "        for sent in sent_tokenize(sec):\n",
    "            tokens.extend([w.lower() for w in word_tokenize(sent)])\n",
    "    return tokens\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "def get_stay_tokens(file, text_dir, keeptime=False):\n",
    "    \"\"\"\n",
    "        input: path\n",
    "        output: tokens in order for all texts representing the stay\n",
    "    \"\"\"\n",
    "    stay_df = pd.read_pickle(os.path.join(text_dir, file))\n",
    "    note_dict = OrderedDict()\n",
    "    for _, row in stay_df.iterrows():\n",
    "        diff = row['DIFFTIME']\n",
    "        if diff < hour:\n",
    "            text = preprocess_mimic(row['TEXT'])\n",
    "            note_dict[diff] = text\n",
    "\n",
    "    if keeptime:\n",
    "        return note_dict\n",
    "    else:\n",
    "        tokens = [t for note in note_dict.values() for t in note]\n",
    "        return tokens\n",
    "\n",
    "def get_common(files, text_dir, output_dir):\n",
    "    all_tokens=[]\n",
    "    for file in tqdm(files):\n",
    "        all_tokens.extend(get_stay_tokens(file, text_dir))\n",
    "    token_count = Counter(all_tokens)\n",
    "\n",
    "    common = [w for (w,c) in token_count.most_common() if c >= args.word_min_freq]  \n",
    "    print(\"{} tokens in text, {} unique, and {} of them appeared at least three times\".format(len(all_tokens), len(token_count),len(common)))\n",
    "    with open(os.path.join(output_dir, 'unique_common.txt'), 'w') as f:\n",
    "        for w in common:\n",
    "            f.write(w+'\\n')\n",
    "    return common\n",
    "\n",
    "def get_embeddings(words, output_dir):\n",
    "    print(\"loading biovec...\")\n",
    "    model = KeyedVectors.load_word2vec_format(os.path.join(args.pretrained_embed_dir, 'BioWordVec_PubMed_MIMICIII_d200.vec.bin'), binary=True)\n",
    "    print(\"loaded, start to get embed for tokens\")\n",
    "\n",
    "    model_vocab = set(model.index_to_key)\n",
    "\n",
    "    valid_words = []\n",
    "    oov = []\n",
    "    for w in words:\n",
    "        if w in model_vocab:\n",
    "            valid_words.append(w)\n",
    "        else:\n",
    "            oov.append(w)\n",
    "    print(\"oov\", oov)\n",
    "\n",
    "    # vocab dicts\n",
    "    token2id = {}\n",
    "    token2id['<pad>'] = 0\n",
    "    for word in valid_words:\n",
    "        token2id[word] = len(token2id)\n",
    "    token2id['<unk>'] = len(token2id)\n",
    "\n",
    "    # get embeddings; pad initiliazed as zero, unk as random\n",
    "    dim = model.vectors.shape[1]\n",
    "    embedding = np.zeros( (len(valid_words)+2, dim), dtype=np.float32)\n",
    "    embedding[0] = np.zeros(dim,)\n",
    "    embedding[-1] = np.random.randn(dim,)\n",
    "    print(\"embed shape\", embedding.shape)\n",
    "    for i, w in enumerate(valid_words):\n",
    "        embedding[i+1] = model[w]\n",
    "\n",
    "    # save them\n",
    "    t2i_path = os.path.join(output_dir, 'token2id.dict')\n",
    "    with open(t2i_path, 'wb') as f:\n",
    "        pk.dump(token2id, f)\n",
    "\n",
    "    embed_path = os.path.join(output_dir, 'embedding.npy')\n",
    "    np.save(embed_path, embedding)\n",
    "\n",
    "    return token2id\n",
    "\n",
    "def save2id(file, token2id, text_dir, output_dir):\n",
    "    output_path = os.path.join(output_dir, file.replace('pk','dict'))\n",
    "    note_dict = get_stay_tokens(file, text_dir, keeptime=True)\n",
    "\n",
    "    out_dict = OrderedDict()\n",
    "    for key, tokens in note_dict.items():\n",
    "        out_dict[key] = [token2id[w] if w in token2id else token2id['<unk>'] for w in tokens]\n",
    "\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pk.dump(out_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/19792 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19792/19792 [31:14<00:00, 10.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39386136 tokens in text, 241276 unique, and 82086 of them appeared at least three times\n",
      "loading biovec...\n",
      "loaded, start to get embed for tokens\n",
      "oov ['....', 'presyncope.', 'yfgga', \"w'\", '.....', 'q\\\\a7/', 'nu3fy', 'dhuorq', 'dxfdl', 'gg4g~', 's|f', 'w\\\\r', 'v2j/d', '=srgb', 'fevers/chills.', 'bipap.', 'etoh.', 'brbpr.', 'rvr.', 'egd.', 'normalized.', 'worsened.', '.......', 'fff.ff6', 'sbps.', 'neuromonitoring.', 'manegment.', 'west-inr', '108/50.', 'asymptomatic.', 'hypoxic.', 'workup.', '60/p.', 'diaphoresis.', '170s.', 'cc7.', '100s.', 'w/dr.', '3l.', 'prbcs.', 'dnr/dni.', 'lightheadedness.', '5.41.', 'urinal.', 'rigors.', '7.45/36/109.', 'ivfs.', '3hrs/day.', 'levaquin.', 'moved.', 'a1c:5', 'hematocrits.', 'unrevealing.', 'piv.', 'full.', 'milrinone.', 'illicits.', 'worsening.', 'arf.', '7.33/85/68/47.', 'ivx1.', 'daughters.', 'fmask.', 'vigorous.', 'vna.', 'sunday.', 'stairs.', 'trached.', 'initially.', 'catherization.', 'pivs.', 'tte/pfts/dental', 'habitus.', 'worsen.', '.17=cjpxyabh', 'methimazole.', 'b/l.', 'downward.', 'blowout.', 'interactive.']\n",
      "embed shape (82008, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 5475/19792 [09:04<31:14,  7.64it/s]  "
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'data'\n",
    "MIMIC_PATH = 'D:\\\\codes\\\\mimic-iii-clinical-database-1.4'\n",
    "EMBED_PATH = 'embedding'\n",
    "CHECKPOINT_PATH_ms_model1 = 'checkpoints/ms_model1'\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "args = {\n",
    "    'eval_model': CHECKPOINT_PATH_ms_model1,\n",
    "    'cohort': 'ms',\n",
    "    'data_dir': DATA_PATH,\n",
    "    'model': 'CAML',\n",
    "    'dropout_rate': 0.3,\n",
    "    'rule': 13,\n",
    "    'rule_dir': 'rules',\n",
    "    'max_seq_length': 2000,\n",
    "    'cnn_filter_maps': 256,\n",
    "    'single_kernel_size': 5,\n",
    "    'batch_size': 32,\n",
    "    # 'target': 'rw',\n",
    "    'target': 'drg',\n",
    "    'device': 'cuda',\n",
    "    'Y': 738,\n",
    "    'threshold': 48,\n",
    "    'word_min_freq': 3,\n",
    "    'pretrained_embed_dir': 'embedding'\n",
    "    }\n",
    "\n",
    "\n",
    "args = dotdict(args)\n",
    "\n",
    "hour=args.threshold\n",
    "data_dir = '%s/%s' % (args.data_dir, args.cohort)\n",
    "text_dir = '%s/text_raw' % data_dir\n",
    "output_dir = '%s/text_embed' % data_dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "files = [f for f in os.listdir(text_dir) if f.endswith('pk')]\n",
    "\n",
    "# get vocab and save embeddings\n",
    "words = get_common(files, text_dir, output_dir)\n",
    "token2id = get_embeddings(words, output_dir)\n",
    "\n",
    "# text to ids\n",
    "for file in tqdm(files):\n",
    "    save2id(file, token2id, text_dir, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_SALUD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
